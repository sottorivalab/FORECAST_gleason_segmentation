{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook for training Gleason Segmentation Model\n",
    "#\n",
    "# Based upon pytorch_resnet18_unet.ipynb by Naoto Usuyama\n",
    "#\n",
    "# http://github.com/usuyama/pytorch-unet/\n",
    "#\n",
    "# Original source code licensed under MIT License:\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "class GlandDataset(Dataset):\n",
    "    def __init__(self, root_dir, net_tile_size=(500, 500), labels=['Mask'], levels=[1, 2, 3], transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mask_paths = glob.glob(os.path.join(root_dir, '*', 'Level' + str(levels[0]), '*.png'))\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.levels = levels\n",
    "        \n",
    "        self.net_tile_size = net_tile_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mask_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_root, image_name = os.path.split(self.mask_paths[idx])\n",
    "        image_root = os.path.dirname(image_root)\n",
    "        \n",
    "        mask = -np.ones(self.net_tile_size, dtype=np.int64)\n",
    "        \n",
    "        for i in range(len(self.labels)):\n",
    "            mask_path = os.path.join(image_root, self.labels[i], image_name)\n",
    "            \n",
    "            if os.path.exists(mask_path):\n",
    "                mask_part = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)/255\n",
    "                if mask_part.shape[0] != self.net_tile_size[0] or mask_part.shape[1] != self.net_tile_size[1]:\n",
    "                    mask_part = cv2.resize(mask_part, self.net_tile_size, interpolation = cv2.INTER_NEAREST)\n",
    "            else:\n",
    "                mask_part = np.zeros(self.net_tile_size).astype(np.float32)\n",
    "            \n",
    "            mask[mask_part>0] = i\n",
    "            \n",
    "        mask = mask+1\n",
    "            \n",
    "        image = ()\n",
    "        \n",
    "        for i in range(len(self.levels)):\n",
    "            image_path = os.path.join(image_root, 'Level' + str(self.levels[i]), image_name)\n",
    "            im_part = cv2.imread(image_path)\n",
    "            if im_part.shape[0] != self.net_tile_size[0] or im_part.shape[1] != self.net_tile_size[1]:\n",
    "                im_part = cv2.resize(im_part, self.net_tile_size, interpolation = cv2.INTER_AREA)\n",
    "            im_part = im_part[:, :, ::-1].astype(np.float32)/255\n",
    "            \n",
    "            image = image + (im_part, )\n",
    "            \n",
    "        image = np.concatenate(image, axis=2)\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "lev = [0, 1]\n",
    "lab = ['Normal', 'PIN', 'Gleason3', 'Gleason4', 'Gleason5']\n",
    "\n",
    "train_set = GlandDataset('../../training_data/new_patches/Training/', net_tile_size=(224, 224), levels=lev, labels=lab, transform = trans)\n",
    "val_set = GlandDataset('../../training_data/new_patches/Validation/', net_tile_size=(224, 224), levels=lev, labels=lab, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class, n_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_in_features = [n_channels, 64, 64, 128, 256, 512]\n",
    "        self.base_out_features = [64, 64, 256, 512, 512, 1024]\n",
    "        self.model_features = [512, 512, 256, 128, 64, 64, 64]\n",
    "        \n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        if n_channels != 3:\n",
    "            X = nn.Conv2d(self.level_features[0][0], self.level_features[0][1], kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).cuda()\n",
    "            self.layer0 = nn.Sequential(*([X]+self.base_layers[1:3])) # size=(N, 64, x.H/2, x.W/2)\n",
    "        else:\n",
    "            self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(self.base_in_features[1], self.base_out_features[1], 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 256, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(self.base_in_features[2], self.base_out_features[2], 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 512, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(self.base_in_features[3], self.base_out_features[3], 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 1024, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(self.base_in_features[4], self.base_out_features[4], 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 2048, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(self.base_in_features[5], self.base_out_features[5], 1, 0)\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(self.base_out_features[4] + self.base_out_features[5], self.model_features[0], 3, 1)\n",
    "        self.conv_up2 = convrelu(self.base_out_features[3] + self.model_features[0], self.model_features[1], 3, 1)\n",
    "        self.conv_up1 = convrelu(self.base_out_features[2] + self.model_features[1], self.model_features[2], 3, 1)\n",
    "        self.conv_up0 = convrelu(self.base_out_features[1] + self.model_features[2], self.model_features[3], 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(n_channels, self.model_features[4], 3, 1)\n",
    "        self.conv_original_size1 = convrelu(self.model_features[4], self.model_features[5], 3, 1)\n",
    "        self.conv_original_size2 = convrelu(self.model_features[5] + self.model_features[3], self.model_features[6], 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(self.model_features[6], n_class, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=0.7)\n",
    "    )\n",
    "\n",
    "class ResNetUNetEnsemble(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class, n_models):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_class = n_class\n",
    "        self.n_models = n_models\n",
    "        \n",
    "        self.base_models = nn.ModuleList([ResNetUNet(n_class, 3) for i in range(self.n_models)])\n",
    "        \n",
    "        self.base_in_features = [sum(x) for x in zip(*[model.base_in_features for model in self.base_models])]\n",
    "        self.base_out_features = [sum(x) for x in zip(*[model.base_out_features for model in self.base_models])]\n",
    "        self.model_features = [512, 512, 256, 128, 64, sum([model.model_features[5] for model in self.base_models]), 64]\n",
    "        \n",
    "        self.layer0_1x1 = convrelu(self.base_in_features[1], self.base_out_features[1], 1, 0)\n",
    "        self.layer1_1x1 = convrelu(self.base_in_features[2], self.base_out_features[2], 1, 0)\n",
    "        self.layer2_1x1 = convrelu(self.base_in_features[3], self.base_out_features[3], 1, 0)\n",
    "        self.layer3_1x1 = convrelu(self.base_in_features[4], self.base_out_features[4], 1, 0)\n",
    "        self.layer4_1x1 = convrelu(self.base_in_features[5], self.base_out_features[5], 1, 0)\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(self.base_out_features[4] + self.base_out_features[5], self.model_features[0], 3, 1)\n",
    "        self.conv_up2 = convrelu(self.base_out_features[3] + self.model_features[0], self.model_features[1], 3, 1)\n",
    "        self.conv_up1 = convrelu(self.base_out_features[2] + self.model_features[1], self.model_features[2], 3, 1)\n",
    "        self.conv_up0 = convrelu(self.base_out_features[1] + self.model_features[2], self.model_features[3], 3, 1)\n",
    "        \n",
    "        self.conv_original_size2 = convrelu(self.model_features[5] + self.model_features[3], self.model_features[6], 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(self.model_features[6], n_class, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = [self.base_models[i].conv_original_size0(input[:,(3*i):(3*(i+1)),:,:]) for i in range(self.n_models)]\n",
    "        x_original = [self.base_models[i].conv_original_size1(x_original[i]) for i in range(self.n_models)]\n",
    "        x_original = torch.cat(x_original, dim=1)\n",
    "        \n",
    "        layer0 = [self.base_models[i].layer0(input[:,(3*i):(3*(i+1)),:,:]) for i in range(self.n_models)]\n",
    "        layer1 = [self.base_models[i].layer1(layer0[i]) for i in range(self.n_models)]\n",
    "        layer2 = [self.base_models[i].layer2(layer1[i]) for i in range(self.n_models)]\n",
    "        layer3 = [self.base_models[i].layer3(layer2[i]) for i in range(self.n_models)]\n",
    "        layer4 = [self.base_models[i].layer4(layer3[i]) for i in range(self.n_models)]\n",
    "\n",
    "        layer4 = self.layer4_1x1(torch.cat(layer4, dim=1))\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(torch.cat(layer3, dim=1))\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(torch.cat(layer2, dim=1))\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(torch.cat(layer1, dim=1))\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(torch.cat(layer0, dim=1))\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    def label_to_onehot(inp):\n",
    "        out = torch.zeros((inp.shape[0], num_class, inp.shape[1], inp.shape[2])).cuda()\n",
    "    \n",
    "        for i in range(0,num_class):\n",
    "            out[:,i-1,:,:] = (inp==i)\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    w = torch.cuda.FloatTensor([0.5, 1, 1, 1, 1, 1])\n",
    "    loss = F.cross_entropy(pred, target, weight=w, ignore_index=-1)\n",
    "    \n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25, best_loss=1e10):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)      \n",
    "                labels = labels.to(device)       \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "                \n",
    "                time_elapsed = time.time() - since\n",
    "                print('{:d}/{:d} {:.0f}m {:.0f}s, loss: {:.2f}'.format(epoch_samples // inputs.size(0), len(dataloaders[phase]), time_elapsed // 60, time_elapsed % 60, loss), end='\\r')\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = len(train_set.labels)+1\n",
    "num_models = len(train_set.levels)\n",
    "model = ResNetUNetEnsemble(n_class=num_class, n_models=num_models).to(device)\n",
    "\n",
    "best_loss = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1)        \n",
    "       \n",
    "model, best_loss = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=5, best_loss=best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp[:,:,:3]\n",
    "\n",
    "def label_to_onehot(inp):\n",
    "    out = np.zeros(inp.shape + (num_class, ))\n",
    "    \n",
    "    for i in range(0,num_class):\n",
    "        out[:,:,i-1] = (inp==i)\n",
    "    \n",
    "    out = out.transpose([2, 0, 1])\n",
    "    \n",
    "    return out\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    " \n",
    "inputs, labels = next(iter(dataloaders['val']))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = F.softmax(model(inputs), dim=1)\n",
    "pred = pred.data.cpu().numpy()\n",
    "pred = np.argmax(pred, axis=1)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(label_to_onehot(x)) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(label_to_onehot(x)) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_location = '../../models/NewModel.h5'\n",
    "os.makedirs(os.path.dirname(save_location), exist_ok=True)\n",
    "\n",
    "torch.save(model, save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_location = '../../models/NewModel.h5'\n",
    "\n",
    "model = torch.load(load_location).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
